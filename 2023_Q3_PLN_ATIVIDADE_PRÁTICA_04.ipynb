{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vini-castro/PLNATV4_ViniCastro/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/11 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:` Vinicius Pelogga de Castro, 11201811430\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: ` 7\n",
        "\n",
        "`Segundo capítulo:` 18\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 -q\n",
        "!pip install spellchecker -q\n",
        "!pip install pyspellchecker -q"
      ],
      "metadata": {
        "id": "N9mIUpmJVJyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b891aca-6aa9-42a3-9a13-c758cb9e5330"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spellchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for inexactsearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for silpa_common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for soundex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minha_chave = ''"
      ],
      "metadata": {
        "id": "f_jlfA704B5L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "response = requests.get('https://brasileiraspln.com/livro-pln/1a-edicao/parte4/cap7/cap7.html')\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "paragrafos = soup.find_all('p')\n",
        "\n",
        "tokens_aux = []\n",
        "palavras_corrigidas = []\n",
        "associacoes = {}\n",
        "\n",
        "for paragrafo in paragrafos:\n",
        "    texto = paragrafo.text\n",
        "    palavras = texto.split()\n",
        "    tokens_aux.append(palavras)\n",
        "\n",
        "tokens = []\n",
        "for x in tokens_aux:\n",
        "    tokens.extend(x)\n",
        "\n",
        "texto_completo = ' '.join(tokens)\n",
        "\n",
        "tamanho = len(texto_completo) // 2\n",
        "texto_completo1 = texto_completo[:tamanho]"
      ],
      "metadata": {
        "id": "6liifyd5_mmr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "response = requests.get('https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap18/cap18.html')\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "paragrafos = soup.find_all('p')\n",
        "\n",
        "tokens_aux = []\n",
        "palavras_corrigidas = []\n",
        "associacoes = {}\n",
        "\n",
        "for paragrafo in paragrafos:\n",
        "    texto = paragrafo.text\n",
        "    palavras = texto.split()\n",
        "    tokens_aux.append(palavras)\n",
        "\n",
        "tokens = []\n",
        "for x in tokens_aux:\n",
        "    tokens.extend(x)\n",
        "\n",
        "texto_completo = ' '.join(tokens)\n",
        "\n",
        "tamanho = len(texto_completo) // 16\n",
        "texto_completo1 = texto_completo[:tamanho]"
      ],
      "metadata": {
        "id": "yvFnA0UZZfJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correção Ortográfica**\n",
        "\n",
        "No caso a correção foi feita em apenas 1 parte do texto, pois a API do Chat Completions tem um limite maximo de caracteres, mas o mesmo seria aplicado para as outras partes, e o resultado seria o mesmo"
      ],
      "metadata": {
        "id": "nVFrf95mM3mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 7:"
      ],
      "metadata": {
        "id": "Er2OAv6uGYGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Faça uma correção ortografica no seguinte texto:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "tokens_texto_completo1 = texto_completo1.split()\n",
        "tokens_resposta = texto_resposta.split()\n",
        "\n",
        "print('Texto completo:\\n'+texto_completo1+'\\n')\n",
        "print('Texto resposta:\\n'+texto_resposta+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El6hothMkU3j",
        "outputId": "d3f8db1b-7779-4850-8263-37b992accc94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto completo:\n",
            "Elisa Terumi Rubel Schneider Adriana S Pagano Ana Clara S Pagano 26/09/2023 PDF A sintaxe é o nível de análise linguística no qual examinamos os padrões de estruturação de sentenças. Isto é, analisamos como as palavras se organizam em unidades que constroem significado dentro da sentença. Para isso, consideramos a classe de cada palavra, sua ordem na sentença e sua relação com as outras palavras. Conforme visto no Capítulo 6, em PLN, a análise computacional realizada no nível sintático é denominada parsing, a ferramenta que realiza essa tarefa é denominada parser e o recurso criado por meio da análise sintática é chamado treebank. Neste Capítulo, vamos conhecer tipos de parsing sob a perspectiva computacional, juntamente com ferramentas e recursos disponíveis para o processamento do português brasileiro. A tarefa de parsing consiste em, dada uma entrada com uma sentença sem nenhuma anotação (raw), um modelo faz uma predição da estrutura sintática dessa sentença. Como vimos no Capítulo 6, o objetivo do processamento sintático é identificar as unidades (como palavras, sintagmas e orações) na sentença e estabelecer as relações gramaticais entre elas a fim de extrair algum tipo de informação. Essas relações podem ser analisadas em termos de: Assim, de acordo com o tipo de análise sintática adotada, há parsers de constituência e parsers de dependência. Mas há uma perspectiva adicional sob a qual podemos caracterizar tipos de parsing e parsers: trata-se do escopo ou profundidade com que a análise sintática é executada. Nesse sentido, podemos analisar as sentenças de forma exaustiva até obtermos uma análise completa de sua estrutura ou fazer uma análise mais rasa para obtermos uma análise com informações mínimas, mas relevantes para as tarefas em PLN. O primeiro tipo é denominado deep (em português, profundo) ou parsing completo e o segundo tipo é denominado shallow (em português, superficial) ou parsing parcial. Contudo, cabe uma observação sobre esta terminologia. No uso geral, os termos parsing e parser acabaram sendo adotados para se referir ao parsing completo. Já o parsing parcial é conhecido como chunking (em português, cortar) e a ferramenta como chunker, embora chunking seja uma dentre várias abordagens para a implementação do parsing parcial (Jurafsky; Martin, 2023). Tanto o parsing de constituência como o parsing de dependência podem ser executados de forma completa ou parcial. Tomando como exemplo o parsing de constituência, uma análise completa ou deep extrai todos os agrupamentos e as relações sintáticas em uma sentença. Por exemplo, dada a sentença: Exemplo 7.1 Emma Watson postou uma foto com Daniel Radcliffe. Temos na Figura 7.1 uma representação em diagrama de árvore que mostra a profundidade da análise. Já uma análise parcial extrai constituintes delimitados, sem estabelecer a hierarquia entre eles ou de que forma uns estão contidos em outros. A Figura 7.2 ilustra a análise rasa, não hierárquica do parsing parcial para o Exemplo 7.1. O objetivo do parsing parcial é gerar uma representação rasa da estrutura da sentença que possibilite um processamento mais rápido de grandes volumes de texto. É geralmente implementado por meio de tokenização de uma sentença em palavras, identificação da classe de palavra (PoS) e segmentação em pedaços ou chunks. O conceito de chunk foi proposto por Abney (1992) como uma unidade formada por uma única palavra ou por um conjunto de palavras. Em um chunk, há uma palavra de conteúdo circundada por palavras funcionais. A palavra de conteúdo mais explorada em chunking é o substantivo, dada a alta correlação de substantivos com entidades. Assim, a tarefa de chunking da sentença do Exemplo 7.1, executada em Python utilizando o modelo de língua portuguesa pt_core_news_sm, da biblioteca spaCy, gera o resultado disposto na Figura 7.3: Como vemos na Figura 7.3, o chunking reconhece três unidades ou chunks, cada uma nucleada por um substantivo. Os três chunks são candidatos a entidades, sendo duas delas nomes próprios de pessoas. De fato, como veremos no Capítulo 17, vários modelos de Extração de Informação utilizam análises rasas como a fornecida pelo chunking. Nesta seção, vamos conhecer alguns recursos e ferramentas de PLN para análise sintática do português. O primeiro recurso para o processamento linguístico é um corpus anotado ou treebank, isto é, textos enriquecidos com marcações de classe de palavras (Part-of-Speech) e relações sintáticas. Um exemplo de corpus em português anotado é o Bosque1, amplamente utilizado para treinar modelos de análise sintática (Veja Capítulo 14). O corpus Bosque é parte de um corpus maior, chamado Floresta Sintá(c)tica2, que abrange, além do Bosque, outros subcorpora, nomeadamente: Selva, Amazônia e Floresta Virgem. O grande corpus foi anotado automaticamente pelo parser PALAVRAS (Bick, 2000). O Bosque está integrado por sentenças extraídas dos corpora CETENFolha (português brasileiro) e CETEMPúblico (português europeu), ambos constituídos por textos jornalísticos escritos. Uma versão do Bosque3 foi convertida para o formato UD (Universal Dependencies), apresentado no Capítulo 6, e é hoje um dos treebanks mais utilizados pela comunidade de PLN no Brasil em modelos de parsing de dependência atuais. Além da Floresta Sintá(c)tica, encontra-se disponível, como recurso para a língua portuguesa, o Corpus Internacional do Português – CINTIL4, desenvolvido pela Universidade de Lisboa, que possui 1 milhão de tokens de texto jornalístico, com anotação de classe de palavra, lema e expressões multipalavra. Uma versão desse corpus, o CINTIL-UDep5, é disponibilizada com anotações no padrão UD. Mais recentemente, o corpus PetroGold6 foi disponibilizado e, hoje, é um corpus passível de ser utilizado em modelos de parsing de dependência. PetroGold é um corpus de textos acadêmicos no domínio do petróleo, anotado no formato UD e revisado manualmente. Há diversas iniciativas em andamento, no momento da escrita deste capítulo, para a criação de corpora anotados em português brasileiro. No escopo do projeto NLP2, desenvolvido pelo Centro de Inteligência Artificial7 (C4A1) da Universidade de São Paulo, com o objetivo de desenvolver recursos, ferramentas e aplicações para levar o português ao estado da arte em PLN, o projeto POeTiSA8 desenvolve o treebank Porttinari9, um corpus multi-gênero de textos em português brasiliero anotados de acordo com o padrão UD. Inclui textos jornalísticos do corpus da Folha de São Paulo/Kaggle, o corpus MAC-MORPHO10 de textos jornalísticos, o corpus DANTE11 (Dependency-ANalised corpora of TwEets), integrado por tweets da Bolsa de Valores, B2W-reviews0112, composto por resenhas e avaliações de consumidores da empresa de comércio eletrônico Americanas e um corpus de Resenhas online de livros. A versão Porttinari-base já se encontra disponível13. Uma iniciativa também em andamento é o corpus Veredas14, desenvolvido na Faculdade de Letras da UFMG, que visa à construção de treebanks de textos anotados de acordo com o padrão das UD. Inclui amostras de uma variedade de textos em inglês, espanhol e português brasileiro: colunas jornalísticas, fábulas, narrativas, receitas culinárias, questionários médicos e bulas de medicamento. Em parceria com a PUCPR, a Faculdade de Letras da UFMG desenvolveu o treebank DepClinBr, um corpus de narrativas clínicas anotadas de acordo com o padrão das UD (Oliveira et al., 2022). Parsers são ferramentas que podem auxiliar uma aplicação (por exemplo, tradução automática, sumarização de textos, extração de informação, question-answering) ou fazer parte de uma ferramenta maior ou conjunto de ferramentas (toolkit). Algumas ferramentas computacionais estão disponíveis para realizar a análise sintática em português. A análise pode ser feita por meio de: Existem diversos parsers desenvolvidos por distintos grupos de pesquisa, fornecidos através de um programa de computador ou aplicativo a ser instalado. No Brasil, podemos citar Curupira15, Donatus16 e PassPort17. Curupira é um analisador robusto de uso geral para o português brasileiro, fornecendo um conjunto das análises sintáticas possíveis para uma frase de entrada. A ferramenta analisa sentenças de cima para baixo, da esquerda para a direita, por meio de uma gramática funcional livre de contexto, restrita e relaxada, para o português brasileiro escrito padrão e um léxico extenso e de ampla cobertura. A Figura 7.4 apresenta a interface gráfica onde é possível ver a obtenção de toda informação da análise realizada pelas regras do parser18. Fonte: (Martins; Nunes; Hasegawa, 2003) Donatus é um projeto que consiste em ferramentas e gramáticas baseadas em Python e na biblioteca NLTK19 para análise profunda e anotação sintática de corpora do português brasileiro. Inclui uma interface gráfica, conforme pode ser visto na Figura 7.5. Está disponível em repositório público, sob licença GNU General Public License version 3.0 (GPLv3). Fonte: (Alencar, 2012) PassPort é uma ferramenta para análise de dependências de português treinado com o Stanford Parser, utilizando o corpus Portuguese Universal Dependency (PT-UD). Infelizmente, a página do projeto não se encontra disponível na data de escrita deste capítulo. Devido à popularidade de Python, muitas bibliotecas de PLN foram desenvolvidas na linguagem. Entre as bibliotecas que incluem parsing para língua portuguesa, podemos citar spaCy20, Stanza21 e NLTK. spaCy é uma biblioteca PLN que oferece análise linguística eficiente e rápida para várias línguas, incluindo o português. Inclui recursos para tokenização, marcação de parte do discurso (PoS tagging), reconhecimento de entidades nomeadas, análise sintática e outros. Através de modelos pré-treinados, o spaCy é capaz de fornecer análises detalhadas, permitindo a extração de informações semânticas de um texto em língua portuguesa. Stanza22 é outra biblioteca PLN que suporta vários idiomas, incluindo o português, desenvolvida pela Universidade Stanford. Fornece uma gama de recursos semelhantes ao spaCy, com suporte a análises mais profundas, como a análise de dependência neural. NLTK (Natural \n",
            "\n",
            "Texto resposta:\n",
            "Arrumação dos nomes; Elisa Terumi Rubel Schneider, Adriana S. Pagano, Ana Clara S. Pagano. 26/09/2023 PDF A sintaxe é o nível de análise linguística no qual examinamos os padrões de estruturação de sentenças. Isto é, analisamos como as palavras se organizam em unidades que constroem significado dentro da sentença. Para isso, consideramos a classe de cada palavra, sua ordem na sentença e sua relação com as outras palavras. Conforme visto no Capítulo 6, em PLN, a análise computacional realizada no nível sintático é denominada parsing, a ferramenta que realiza essa tarefa é denominada parser e o recurso criado por meio da análise sintática é chamado treebank. Neste Capítulo, vamos conhecer tipos de parsing sob a perspectiva computacional, juntamente com ferramentas e recursos disponíveis para o processamento do português brasileiro. A tarefa de parsing consiste em, dada uma entrada com uma sentença sem nenhuma anotação (raw), um modelo faz uma predição da estrutura sintática dessa sentença. Como vimos no Capítulo 6, o objetivo do processamento sintático é identificar as unidades (como palavras, sintagmas e orações) na sentença e estabelecer as relações gramaticais entre elas a fim de extrair algum tipo de informação. Essas relações podem ser analisadas em termos de: Assim, de acordo com o tipo de análise sintática adotada, há parsers de constituência e parsers de dependência. Mas há uma perspectiva adicional sob a qual podemos caracterizar tipos de parsing e parsers: trata-se do escopo ou profundidade com que a análise sintática é executada. Nesse sentido, podemos analisar as sentenças de forma exaustiva até obtermos uma análise completa de sua estrutura ou fazer uma análise mais rasa para obtermos uma análise com informações mínimas, mas relevantes para as tarefas em PLN. O primeiro tipo é denominado deep (em português, profundo) ou parsing completo e o segundo tipo é denominado shallow (em português, superficial) ou parsing parcial. Contudo, cabe uma observação sobre esta terminologia. No uso geral, os termos parsing e parser acabaram sendo adotados para se referir ao parsing completo. Já o parsing parcial é conhecido como chunking (em português, cortar) e a ferramenta como chunker, embora chunking seja uma dentre várias abordagens para a implementação do parsing parcial (Jurafsky, Martin, 2023). Tanto o parsing de constituência como o parsing de dependência podem ser executados de forma completa ou parcial. Tomando como exemplo o parsing de constituência, uma análise completa ou deep extrai todos os agrupamentos e as relações sintáticas em uma sentença. Por exemplo, dada a sentença: Exemplo 7.1 Emma Watson postou uma foto com Daniel Radcliffe. Temos na Figura 7.1 uma representação em diagrama de árvore que mostra a profundidade da análise. Já uma análise parcial extrai constituintes delimitados, sem estabelecer a hierarquia entre eles ou de que forma uns estão contidos em outros. A Figura 7.2 ilustra a análise rasa, não hierárquica do parsing parcial para o Exemplo 7.1. O objetivo do parsing parcial é gerar uma representação rasa da estrutura da sentença que possibilite um processamento mais rápido de grandes volumes de texto. É geralmente implementado por meio de tokenização de uma sentença em palavras, identificação da classe de palavra (PoS) e segmentação em pedaços ou chunks. O conceito de chunk foi proposto por Abney (1992) como uma unidade formada por uma única palavra ou por um conjunto de palavras. Em um chunk, há uma palavra de conteúdo circundada por palavras funcionais. A palavra de conteúdo mais explorada em chunking é o substantivo, dada a alta correlação de substantivos com entidades. Assim, a tarefa de chunking da sentença do Exemplo 7.1, executada em Python utilizando o modelo de língua portuguesa pt_core_news_sm, da biblioteca spaCy, gera o resultado disposto na Figura 7.3: Como vemos na Figura 7.3, o chunking reconhece três unidades ou chunks, cada uma nucleada por um substantivo. Os três chunks são candidatos a entidades, sendo duas delas nomes próprios de pessoas. De fato, como veremos no Capítulo 17, vários modelos de Extração de Informação utilizam análises rasas como a fornecida pelo chunking. Nesta seção, vamos conhecer alguns recursos e ferramentas de PLN para análise sintática do português. O primeiro recurso para o processamento linguístico é um corpus anotado ou treebank, isto é, textos enriquecidos com marcações de classe de palavras (Part-of-Speech) e relações sintáticas. Um exemplo de corpus em português anotado é o Bosque1, amplamente utilizado para treinar modelos de análise sintática (Veja Capítulo 14). O corpus Bosque é parte de um corpus maior, chamado Floresta Sintá(c)tica2, que abrange, além do Bosque, outros subcorpora, nomeadamente: Selva, Amazônia e Floresta Virgem. O grande corpus foi anotado automaticamente pelo parser PALAVRAS (Bick, 2000). O Bosque está integrado por sentenças extraídas dos corpora CETENFolha (português brasileiro) e CETEMPúblico (português europeu), ambos constituídos por textos jornalísticos escritos. Uma versão do Bosque3 foi convertida para o formato UD (Universal Dependencies), apresentado no Capítulo 6, e é hoje um dos treebanks mais utilizados pela comunidade de PLN no Brasil em modelos de parsing de dependência atuais. Além da Floresta Sintá(c)tica, encontra-se disponível, como recurso para a língua portuguesa, o Corpus Internacional do Português – CINTIL4, desenvolvido pela Universidade de Lisboa, que possui 1 milhão de tokens de texto jornalístico, com anotação de classe de palavra, lema e expressões multipalavra. Uma versão desse corpus, o CINTIL-UDep5, é disponibilizada com anotações no padrão UD. Mais recentemente, o corpus PetroGold6 foi disponibilizado e, hoje, é um corpus passível de ser utilizado em modelos de parsing de dependência. PetroGold é um corpus de textos acadêmicos no domínio do petróleo, anotado no formato UD e revisado manualmente. Há diversas iniciativas em andamento, no momento da escrita deste capítulo, para a criação de corpora anotados em português brasileiro. No escopo do projeto NLP2, desenvolvido pelo Centro de Inteligência Artificial7 (C4A1) da Universidade de São Paulo, com o objetivo de desenvolver recursos, ferramentas e aplicações para levar o português ao estado da arte em PLN, o projeto POeTiSA8 desenvolve o treebank Porttinari9, um corpus multi-gênero de textos em português brasiliero anotados de acordo com o padrão UD. Inclui textos jornalísticos do corpus da Folha de São Paulo/Kaggle, o corpus MAC-MORPHO10 de textos jornalísticos, o corpus DANTE11 (Dependency-ANalised corpora of TwEets), integrado por tweets da Bolsa de Valores, B2W-reviews0112, composto por resenhas e avaliações de consumidores da empresa de comércio eletrônico Americanas e um corpus de Resenhas online de livros. A versão Porttinari-base já se encontra disponível13. Uma iniciativa também em andamento é o corpus Veredas14, desenvolvido na Faculdade de Letras da UFMG, que visa à construção de treebanks de textos anotados de acordo com o padrão das UD. Inclui amostras de uma variedade de textos em inglês, espanhol e português brasileiro: colunas jornalísticas, fábulas, narrativas, receitas culinárias, questionários médicos e bulas de medicamento. Em parceria com a PUCPR, a Faculdade de Letras da UFMG desenvolveu o treebank DepClinBr, um corpus de narrativas clínicas anotadas de acordo com o padrão das UD (Oliveira et al., 2022). Parsers são ferramentas que podem auxiliar uma aplicação (por exemplo, tradução automática, sumarização de textos, extração de informação, question-answering) ou fazer parte de uma ferramenta maior ou conjunto de ferramentas (toolkit). Algumas ferramentas computacionais estão disponíveis para realizar a análise sintática em português. A análise pode ser feita por meio de: Existem diversos parsers desenvolvidos por distintos grupos de pesquisa, fornecidos através de um programa de computador ou aplicativo a ser instalado. No Brasil, podemos citar Curupira15, Donatus16 e PassPort17. Curupira é um analisador robusto de uso geral para o português brasileiro, fornecendo um conjunto das análises sintáticas possíveis para uma frase de entrada. A ferramenta analisa sentenças de cima para baixo, da esquerda para a direita, por meio de uma gramática funcional livre de contexto, restrita e relaxada, para o português brasileiro escrito padrão e um léxico extenso e de ampla cobertura. A Figura 7.4 apresenta a interface gráfica onde é possível ver a obtenção de toda informação da análise realizada pelas regras do parser18. Fonte: (Martins; Nunes; Hasegawa, 2003) Donatus é um projeto que consiste em ferramentas e gramáticas baseadas em Python e na biblioteca NLTK19 para análise profunda e anotação sintática de corpora do português brasileiro. Inclui uma interface gráfica, conforme pode ser visto na Figura 7.5. Está disponível em repositório público, sob licença GNU General Public License version 3.0 (GPLv3). Fonte: (Alencar, 2012) PassPort é uma ferramenta para análise de dependências de português treinado com o Stanford Parser, utilizando o corpus Portuguese Universal Dependency (PT-UD). Infelizmente, a página do projeto não se encontra disponível na data de escrita deste capítulo. Devido à popularidade de Python, muitas bibliotecas de PLN foram desenvolvidas na linguagem. Entre as bibliotecas que incluem parsing para língua portuguesa, podemos citar spaCy20, Stanza21 e NLTK. spaCy é uma biblioteca PLN que oferece análise linguística eficiente e rápida para várias línguas, incluindo o português. Inclui recursos para tokenização, marcação de parte do discurso (PoS tagging), reconhecimento de entidades nomeadas, análise sintática e outros. Através de modelos pré-treinados, o spaCy é capaz de fornecer análises detalhadas, permitindo a extração de informações semânticas de um texto em língua portuguesa. Stanza22 é outra biblioteca PLN que suporta vários idiomas, incluindo o português, desenvolvida pela Universidade Stanford. Fornece uma gama de recursos semelhantes ao spaCy, com suporte a análises mais profundas, como a análise de dependência neural. NLTK (Natural\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 18:"
      ],
      "metadata": {
        "id": "w8dIBwc7Gao_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Faça uma correção ortografica no seguinte texto:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "tokens_texto_completo1 = texto_completo1.split()\n",
        "tokens_resposta = texto_resposta.split()\n",
        "\n",
        "palavras_diferentes = set(tokens_texto_completo1) ^ set(tokens_resposta)\n",
        "\n",
        "print('Texto completo:\\n'+texto_completo1+'\\n')\n",
        "print('Texto resposta:\\n'+texto_resposta+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_TEdfhQaRxt",
        "outputId": "8133016b-db97-44fb-c831-9852fbe0e4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto completo:\n",
            "Abordagens e Avaliação Sheila Castilho Helena de Medeiros Caseli 26/09/2023 PDF A tradução automática (TA), também conhecida como tradução de máquina (em inglês, machine translation ou MT), refere-se à tradução de um texto eletrônico por um computador de uma língua para outra sem intervenção humana. Nesse sentido, convencionou-se chamar de língua (ou texto) fonte a língua de partida (origem) e língua (ou texto) alvo a língua de chegada (destino ou saída). Além de envolver a análise e interpretação (NLU) da língua-fonte e a geração (NLG) da língua-alvo, há a premissa fundamental de gerar uma saída que seja semanticamente equivalente (transmite o mesmo significado) à entrada. Nos últimos anos, a TA evoluiu significativamente com o avanço de modelos estatísticos e neurais. Atualmente, ela é amplamente utilizada em todo o mundo por governos, indústria da tradução, consumidores finais e em pesquisas em uma variedade de aplicações. Os primeiros sistemas bem-sucedidos de TA datam do final dos anos 1950 e início dos anos 19601, com os experimentos de Georgetown. No entanto, é possível encontrar referências a tentativas de tradução automática no século XVII (Hutchins, 2001). Desde então, diferentes abordagens para a TA foram desenvolvidas, incluindo abordagens baseadas em regras, exemplos, estatísticas e, mais recentemente, a TA neural, apresentadas brevemente nas diversas seções deste capítulo. Hoje em dia, a TA desempenha um papel importante não apenas no âmbito comercial, mas também no âmbito social e político. Ela é amplamente utilizada em diversas aplicações de comunicação, que incluem2: Devido à ampla utilização da TA atualmente, seu impacto pode ser observado em nossa sociedade. Por essa razão, a avaliação da TA (tema da Seção 18.3) tornou-se mais importante, visando garantir a qualidade da tradução. Além da avaliação, este capítulo descreve as principais abordagens para a TA (Seção 18.2). No decorrer deste capítulo, alguns conceitos-chave são explicados para que você possa acompanhar os desenvolvimentos. A tradução automática pode ser realizada de diversas maneiras, desde a mais simples (tradução direta), que envolve a tradução palavra-a-palavra (ou sequência de palavras), até a mais utilizada na atualidade, que é a tradução baseada em redes neurais artificiais (tradução neural). Na trajetória entre a tradução direta e a tradução neural, explicaremos também abordagens intermediárias, como a baseada em regras, a tradução por interlíngua e a tradução estatística. Para tanto, vamos traduzir a sentença “A casa do meu avô é linda.” para o inglês. Como isso aconteceria de acordo com as abordagens mais utilizadas (ou tradicionais) na tradução automática? Na tradução direta ocorre o mapeamento direto de palavras-fonte para palavras-alvo sem passar por outros níveis de análise3. Assim, no nosso exemplo, os caracteres seriam combinados em palavras e cada uma seria mapeada diretamente para seu equivalente em inglês usando, por exemplo, um léxico bilíngue. Utilizando o léxico bilíngue disponível no github do MUSE4 e a tradução palavra-a-palavra, nossa saída seria como apresentado em Exemplo 18.1: Exemplo 18.1 Entrada: A casa do meu avô é linda. Saída: _ house _ my granddad _ beautiful . onde as palavras para as quais não se encontrou um equivalente no léxico consultado foram substituídas por _5. Vejam que neste processo de tradução não há nenhum processamento referente às línguas envolvidas, uma vez que o resultado é obtido via casamento de padrão, seguido da substituição de uma palavra por outra com base em uma lista de pares de palavras. Obviamente a abordagem de tradução direta apresenta diversas limitações, como não ser capaz de lidar com a estrutura (sintaxe) da língua, que, como pode ser visto no Capítulo 6, é fundamental para o tratamento adequado da língua. A tradução direta foi uma das primeiras abordagens a serem investigadas e não é mais utilizada nos tradutores atuais. Como o nome sugere, os sistemas de TA baseados em regras (em inglês, Rule-based Machine Translation ou RBMT) são sistemas baseados em conhecimento desenvolvidos por meio da especificação de regras linguísticas, que levam em consideração morfologia, sintaxe e semântica das línguas envolvidas, além dos léxicos bilíngues de ambos os idiomas de origem e de destino6. Essas regras e léxicos são formuladas e criados manualmente por especialistas em linguagem. A partir desses recursos, a RBMT é capaz de real\n",
            "\n",
            "Texto resposta:\n",
            "Abordagens e Avaliação - Sheila Castilho Helena de Medeiros Caseli - 26/09/2023 - PDF\n",
            "\n",
            "A tradução automática (TA), também conhecida como tradução de máquina (em inglês, machine translation ou MT), refere-se à tradução de um texto eletrônico por um computador de uma língua para outra sem intervenção humana. Nesse sentido, convencionou-se chamar de língua (ou texto) fonte a língua de partida (origem) e língua (ou texto) alvo a língua de chegada (destino ou saída). Além de envolver a análise e interpretação (NLU) da língua-fonte e a geração (NLG) da língua-alvo, há a premissa fundamental de gerar uma saída que seja semanticamente equivalente (transmite o mesmo significado) à entrada.\n",
            "\n",
            "Nos últimos anos, a TA evoluiu significativamente com o avanço de modelos estatísticos e neurais. Atualmente, ela é amplamente utilizada em todo o mundo por governos, indústria da tradução, consumidores finais e em pesquisas em uma variedade de aplicações. Os primeiros sistemas bem-sucedidos de TA datam do final dos anos 1950 e início dos anos 1960, com os experimentos de Georgetown. No entanto, é possível encontrar referências a tentativas de tradução automática no século XVII (Hutchins, 2001). Desde então, diferentes abordagens para a TA foram desenvolvidas, incluindo abordagens baseadas em regras, exemplos, estatísticas e, mais recentemente, a TA neural, apresentadas brevemente nas diversas seções deste capítulo.\n",
            "\n",
            "Hoje em dia, a TA desempenha um papel importante não apenas no âmbito comercial, mas também no âmbito social e político. Ela é amplamente utilizada em diversas aplicações de comunicação, que incluem:\n",
            "\n",
            "Devido à ampla utilização da TA atualmente, seu impacto pode ser observado em nossa sociedade. Por essa razão, a avaliação da TA (tema da Seção 18.3) tornou-se mais importante, visando garantir a qualidade da tradução. Além da avaliação, este capítulo descreve as principais abordagens para a TA (Seção 18.2). No decorrer deste capítulo, alguns conceitos-chave são explicados para que você possa acompanhar os desenvolvimentos.\n",
            "\n",
            "A tradução automática pode ser realizada de diversas maneiras, desde a mais simples (tradução direta), que envolve a tradução palavra a palavra (ou sequência de palavras), até a mais utilizada na atualidade, que é a tradução baseada em redes neurais artificiais (tradução neural). Na trajetória entre a tradução direta e a tradução neural, explicaremos também abordagens intermediárias, como a baseada em regras, a tradução por interlíngua e a tradução estatística. Para tanto, vamos traduzir a sentença “A casa do meu avô é linda.” para o inglês. Como isso aconteceria de acordo com as abordagens mais utilizadas (ou tradicionais) na tradução automática?\n",
            "\n",
            "Na tradução direta ocorre o mapeamento direto de palavras-fonte para palavras-alvo sem passar por outros níveis de análise. Assim, no nosso exemplo, os caracteres seriam combinados em palavras e cada uma seria mapeada diretamente para seu equivalente em inglês usando, por exemplo, um léxico bilíngue. Utilizando o léxico bilíngue disponível no github do MUSE e a tradução palavra a palavra, nossa saída seria como apresentado em Exemplo 18.1:\n",
            "\n",
            "Exemplo 18.1\n",
            "Entrada: A casa do meu avô é linda.\n",
            "Saída: _ house _ my granddad _ beautiful.\n",
            "\n",
            "Onde as palavras para as quais não se encontrou um equivalente no léxico consultado foram substituídas por _.\n",
            "\n",
            "Vejam que neste processo de tradução não há nenhum processamento referente às línguas envolvidas, uma vez que o resultado é obtido via casamento de padrão, seguido da substituição de uma palavra por outra com base em uma lista de pares de palavras. Obviamente a abordagem de tradução direta apresenta diversas limitações, como não ser capaz de lidar com a estrutura (sintaxe) da língua, que, como pode ser visto no Capítulo 6, é fundamental para o tratamento adequado da língua. A tradução direta foi uma das primeiras abordagens a serem investigadas e não é mais utilizada nos tradutores atuais.\n",
            "\n",
            "Como o nome sugere, os sistemas de TA baseados em regras (em inglês, Rule-based Machine Translation ou RBMT) são sistemas baseados em conhecimento desenvolvidos por meio da especificação de regras linguísticas, que levam em consideração morfologia, sintaxe e semântica das línguas envolvidas, além dos léxicos bilíngues de ambos os idiomas de origem e de destino. Essas regras e léxicos são formuladas e criados manualmente por especialistas em linguagem. A partir desses recursos, a RBMT é capaz de reali.\n",
            "\n",
            "Palavras diferentes:\n",
            "{'reali.', 'MUSE4', 'destino6.', 'análise3.', '1960,', 'palavra-a-palavra', '19601,', 'onde', 'beautiful.', 'real', '-', 'destino.', 'beautiful', 'análise.', '.', '_5.', 'MUSE', 'palavra,', 'Onde', 'incluem2:', 'palavra-a-palavra,', 'incluem:', '_.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analise de Sentimentos**\n",
        "É possível analisar que no contexto geral do texto, a analise é neutra, pois se trata de um trecho informativo. Mas em frases em que se explicam as vantagens do modelo, palavras que trazem mais peso ao sentimento aparecem, podendo encontrar resultados diferentes de neutros."
      ],
      "metadata": {
        "id": "kwUS3v9WM1Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 7:"
      ],
      "metadata": {
        "id": "TlFEX8tPGc9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Analise se o sentimento de uma frase é positivo, neutro ou negativo:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXpOtsi6bNY5",
        "outputId": "9729ec1c-17e6-4968-bdef-168cc1a02e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Toolkit) é uma biblioteca PLN popular que oferece uma variedade de ferramentas e recursos para o processamento de linguagem natural, incluindo parsing e análise sintática para o português. \n",
            "\n",
            "Com base na análise do texto fornecido, o sentimento da frase é neutro, já que se trata de uma explicação técnica sobre análise sintática em processamento de linguagem natural, sem transmitir opiniões ou emoções positivas ou negativas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Encontre uma frase do texto em que o sentimento seja positivo:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGYCydIEHS7L",
        "outputId": "f855eb58-bfc7-48f3-bdef-cc9d9bbe08d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existe uma frase positiva no texto que é \"Através de modelos pré-treinados, o spaCy é capaz de fornecer análises detalhadas, permitindo a extração de informações semânticas de um texto em língua portuguesa.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 18:"
      ],
      "metadata": {
        "id": "wYfaIpEIGerd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Analise se o sentimento de uma frase é positivo, neutro ou negativo:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJKNd7OSjA0z",
        "outputId": "01dbe055-7f09-4628-e2b0-ab6f1fa970c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A frase possui um sentimento neutro, pois descreve de forma imparcial e objetiva a evolução da tradução automática ao longo do tempo, apresentando tanto as vantagens quanto as desvantagens das abordagens estatísticas e neurais. Não há expressão de emoção ou opinião na frase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Encontre uma frase do texto em que o sentimento seja positivo:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNqFwMkdK_u8",
        "outputId": "532ddffd-b6ee-433d-f0a2-c171d5f5d865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Os sistemas neurais geraram grande expectativa, especialmente porque a indústria de tradução busca melhorar a qualidade da TA para minimizar custos\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Palavras-chave**"
      ],
      "metadata": {
        "id": "5iriwiPJMy5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 7:"
      ],
      "metadata": {
        "id": "6brL_Fb-Gl3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Faça uma extração das palavras chaves do seguinte texto:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZP8rVqU3-dr",
        "outputId": "a0deac87-4181-44b1-ae87-66ab402e8b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language Toolkit) é uma biblioteca Python popular para processamento de linguagem natural, incluindo recursos para tokenização, análise sintática, marcação de parte do discurso, lematização, e mais. Apesar de ser uma biblioteca amplamente utilizada, a análise sintática disponível para o português na NLTK pode não ter a mesma qualidade e eficiência de bibliotecas mais modernas como spaCy e Stanza. A partir dos tipos de parsing e das ferramentas de análise sintática descritas, é possível observar que a análise sintática é um componente essencial para diversas tarefas em PLN, como tradução automática, sumarização de textos, extração de informação, question-answering, entre outras. Além disso, a presença de ferramentas, recursos e corpora específicos para o português brasileiro reflete o crescimento e a importância do processamento de linguagem natural para a língua portuguesa.\n",
            "\n",
            "Palavras-chave:\n",
            "Elisa Terumi Rubel Schneider, Adriana S Pagano, Ana Clara S Pagano, PDF, sintaxe, análise linguística, estruturação de sentenças, parsing, parser, treebank, português brasileiro, parsing completo, parsing parcial, chunking, corpus anotado, Bosque, CINTIL, UD (Universal Dependencies), PetroGold, NLP2, POeTiSA, Porttinari, Veredas, DepClinBr, parsers, análise sintática, sparkNLP, Curupira, Donatus, PassPort, spaCy, Stanza, NLTK, processamento de linguagem natural, tradução automática, sumarização de texto, extração de informação, question-answering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 18:"
      ],
      "metadata": {
        "id": "WSPbvW5PGn4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Faça uma extração das palavras chaves do seguinte texto:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkNPt0jwA6lQ",
        "outputId": "e6251404-ab70-4b1d-8bb4-1c8676bef894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "palavras-chave:\n",
            "- sistemas de tradução estatística\n",
            "- tradução neural\n",
            "- redes neurais\n",
            "- modelos estatísticos\n",
            "- abordagens estatísticas\n",
            "- idiomas\n",
            "- tradutor automático\n",
            "- Google Tradutor\n",
            "- sistemas de TA neural\n",
            "- redes neurais artificiais\n",
            "- unidades de processamento\n",
            "- neurônio artificial\n",
            "- pesos\n",
            "- aprendizado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tradução Automatica**"
      ],
      "metadata": {
        "id": "MyaxPLePMwtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 7:"
      ],
      "metadata": {
        "id": "RvoB4v9WGpsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Traduza o seguinte texto para o inglês:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "texto_resposta = texto_resposta.split('\\n')\n",
        "\n",
        "print('Texto traduzido:\\n\\n',texto_resposta[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hTGST9CPa2",
        "outputId": "a82049c6-041e-45d6-95b9-12f627dca6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto traduzido:\n",
            "\n",
            " Elisa Terumi Rubel Schneider Adriana S Pagano Ana Clara S Pagano 26/09/2023 PDF The syntax is the level of linguistic analysis in which we examine the patterns of sentence structuring. That is, we analyze how words are organized into units that construct meaning within the sentence. For this, we consider the class of each word, its order in the sentence, and its relationship with the other words. As seen in Chapter 6, in NLP, the computational analysis performed at the syntactic level is called parsing. The tool that performs this task is called a parser and the resource created through syntactic analysis is called a treebank. In this Chapter, we will learn about types of parsing from a computational perspective, along with available tools and resources for processing Brazilian Portuguese. The parsing task consists of, given an input with an unannotated sentence (raw), a model makes a prediction of the syntactic structure of that sentence. As we saw in Chapter 6, the goal of syntactic processing is to identify the units (such as words, phrases, and clauses) in the sentence and establish the grammatical relations between them in order to extract some type of information. These relationships can be analyzed in terms of: Thus, depending on the type of syntactic analysis adopted, there are constituency parsers and dependency parsers. But there is an additional perspective under which we can characterize types of parsing and parsers: namely, the scope or depth at which the syntactic analysis is executed. In this sense, we can analyze the sentences in a comprehensive way until we obtain a complete analysis of their structure, or make a shallower analysis to obtain an analysis with minimal but relevant information for NLP tasks. The first type is called deep (in Portuguese, profundo) or complete parsing and the second type is called shallow (in Portuguese, superficial) or partial parsing. However, an observation is warranted about this terminology. In general use, the terms parsing and parser have ultimately been adopted to refer to complete parsing. Partial parsing is known as chunking, and the tool as a chunker, although chunking is one among several approaches to the implementation of partial parsing (Jurafsky; Martin, 2023). Both constituency parsing and dependency parsing can be executed in a complete or partial manner. Taking constituency parsing as an example, a complete or deep analysis extracts all groupings and syntactic relationships in a sentence. For example, given the sentence: Example 7.1 Emma Watson posted a photo with Daniel Radcliffe. We have in Figure 7.1 a tree diagram representation that shows the depth of the analysis. The objective of partial parsing is to generate a shallow representation of the sentence structure that allows for faster processing of large volumes of text. It is generally implemented through tokenization of a sentence into words, identification of the part of speech (PoS) and segmentation into chunks. The concept of chunk was proposed by Abney (1992) as a unit formed by a single word or by a set of words. In a chunk, there is a content word surrounded by functional words. The most explored content word in chunking is the noun, given the high correlation of nouns with entities. Thus, the chunking task for the sentence from Example 7.1, executed in Python using the pt_core_news_sm Portuguese language model from the spaCy library, generates the result shown in Figure 7.3: As seen in Figure 7.3, the chunking recognizes three units or chunks, each centered on a noun. The three chunks are candidates for entities, with two of them being proper names of individuals. In fact, as we will see in Chapter 17, several Information Extraction models use shallow analyses such as the one provided by chunking. In this section, we will get to know some resources and tools for NLP syntactic analysis of Portuguese. The first resource for language processing is an annotated corpus or treebank, that is, texts enriched with part-of-speech (PoS) and syntactic relations markings. An example of an annotated corpus in Portuguese is the Bosque1, widely used for training syntactic analysis models (See Chapter 14). The Bosque corpus is part of a larger corpus, called Floresta Sintá(c)tica2, which includes, in addition to Bosque, other sub-corpora, namely: Selva, Amazônia, and Floresta Virgem. The large corpus was automatically annotated by the PALAVRAS parser (Bick, 2000). The Bosque corpus is composed of sentences extracted from the CETENFolha (Brazilian Portuguese) and CETEMPúblico (European Portuguese) corpora, both consisting of written journalistic texts. A version of Bosque3 has been converted to the Universal Dependencies (UD) format, presented in Chapter 6, and is today one of the most used treebanks by the NLP community in Brazil in current dependency parsing models. In addition to Floresta Sintá(c)tica, there is also available for Portuguese language processing the International Corpus of Portuguese – CINTIL4, developed by the University of Lisbon, which has 1 million tokens of journalistic text, with annotation of part-of-speech, lemma, and multi-word expressions. A version of this corpus, the CINTIL-UDep5, is made available with UD standard annotations. More recently, the PetroGold6 corpus was made available and is now a corpus that can be used in dependency parsing models. PetroGold is a corpus of academic texts in the domain of petroleum, annotated in UD format and manually revised. There are several ongoing initiatives, at the time of writing this chapter, for the creation of annotated corpora in Brazilian Portuguese. In the scope of the NLP2 project, developed by the Artificial Intelligence Center7 (C4A1) of the University of São Paulo, with the objective of developing resources, tools, and applications to bring Portuguese to the state of the art in NLP, the POeTiSA8 project is developing the Porttinari9 treebank, a multi-genre corpus of Brazilian Portuguese texts annotated according to the UD standard. It includes journalistic texts from the Folha de São Paulo/Kaggle corpus, the MAC-MORPHO10 corpus of journalistic texts, the DANTE11 (Dependency-ANalised corpora of TwEets) corpus, consisting of tweets from the Stock Exchange, B2W-reviews0112, composed of reviews and consumer ratings of the e-commerce company Americanas, and a corpus of online book reviews. The Porttinari-base version is already available13. Another ongoing initiative is the Veredas14 corpus, developed at the Faculty of Letters of UFMG, which aims to construct treebanks of annotated texts according to the UD standard. It includes samples of a variety of texts in English, Spanish, and Brazilian Portuguese: journalistic columns, fables, narratives, recipes, medical questionnaires, and medication leaflets. In partnership with PUCPR, the Faculty of Letters of UFMG developed the DepClinBr treebank, a corpus of clinical narratives annotated according to the UD standard (Oliveira et al., 2022). Parsers are tools that can assist an application (for example, machine translation, text summarization, information extraction, question-answering) or be part of a larger tool or toolkit. Some computational tools are available to perform syntactic analysis in Portuguese. The analysis can be done through: There are several parsers developed by different research groups, provided through a computer program or app to be installed. In Brazil, we can mention Curupira15, Donatus16, and PassPort17. Curupira is a robust general-purpose analyzer for Brazilian Portuguese, providing a set of possible syntactic analyses for an input phrase. The tool analyzes sentences from top to bottom, from left to right, through a free, restricted, and relaxed context-free grammar, for standard written Brazilian Portuguese and an extensive and wide coverage lexicon. The graphic interface in which it is possible to see the obtainment of all information from the analysis performed by the parser rules is shown in Figure 7.4. Source: (Martins; Nunes; Hasegawa, 2003) Donatus is a project consisting of tools and grammars based on Python and the NLTK library19 for deep analysis and syntactic annotation of corpora of Brazilian Portuguese. It includes a graphic interface, as can be seen in Figure 7.5. It is available in a public repository, under the GNU General Public License version 3.0 (GPLv3). Source: (Alencar, 2012) PassPort is a tool for dependency analysis of Portuguese trained with the Stanford Parser, using the Portuguese Universal Dependency (PT-UD) corpus. Unfortunately, the project page is not available at the time of writing this chapter. Due to the popularity of Python, many NLP libraries were developed in the language. Among the libraries that include parsing for the Portuguese language, we can mention spaCy20, Stanza21, and NLTK. spaCy is an NLP library that offers fast and efficient linguistic analysis for several languages, including Portuguese. It includes features for tokenization, part of speech tagging, named entity recognition, syntactic analysis, and more. Through pre-trained models, spaCy can provide detailed analyses, allowing for the extraction of semantic information from a text in Portuguese. Stanza22 is another NLP library that supports various languages, including Portuguese, developed by Stanford University. It provides a range of features similar to spaCy, with support for deeper analyses, such as neural dependency analysis. NLTK (Natural Language Toolkit) is a Python NLP library that provides useful tools for computational linguistics, including resources for syntactic analysis, tokenization, part of speech tagging, and more. It is one of the most widely used libraries for NLP in Python.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cap. 18"
      ],
      "metadata": {
        "id": "wofgEKwkGxOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Traduza o seguinte texto para o inglês:\"+texto_completo1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print('Texto traduzido:\\n\\n',texto_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOMjM6NwEhJV",
        "outputId": "f40b2be9-cd44-4495-e8df-de0095a55153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto traduzido:\n",
            "\n",
            " Could a simplified approach to the process? Yes. To make it clear, imagine the following scenario where there are documents written in n different languages and we want to translate them to and from any of these languages. We could build arrangements of n automatic translators for each pair of languages (n1-n2, n1-n3, n1-n4, ..., n2-n1, n2-n3, n2-n4, ...) following the transfer model (rule-based machine translation). But the interlanguage translation proves to be more advantageous. We can define a metalanguage and divide the task into n groups of experts/translators, each responsible for a single language, li (preferably their mother tongue). It will be up to each group to build a translator (remember: much simpler!), or encoder, from li to the metalanguage, and another, decoder, from the metalanguage to li. Evidently all groups share the same knowledge about the intermediate language. Considering that each group will do its part, in the end, the possible combinations of these translation modules will give rise to all the desired translators. For example, to translate li to lj, we combine the encoder module of li with the decoder module for lj. 2n simpler translation modules will have been built, therefore, less effort than that required for bilingual translators. Another consequence is the possibility of evaluating translations through reverse translation, since the independent modules allow translation in both directions. This ideal has been shared a few times in the past by various academic research groups, but unfortunately practice has revealed several problems. The biggest one is, according to critics, the naivety of believing it possible to create a language capable of representing the meaning of all others, therefore, universal. Another problem - or a result of this - is the unanimous adoption of a given intermediate language by the groups of different languages, where each of them claims alterations and adaptations, revealing the nonexistence of a truly universal core. For these and other reasons, this model did not replace the model for bilingual transfer. In the late 1990s, Brazilian Portuguese was represented, by NILC, in a UN initiative to build translators for the most spoken languages in the world, the UNL Project. This project aimed to develop a multilingual system of automatic translation based on a semantic interlanguage - the Universal Networking Language (UNL) - developed by researchers linked to the United Nations University, an arm of the UN, in Tokyo. The linguistic paradigm (based on rules and interlanguage), in which linguistic knowledge is explicitly mapped into resources such as rules, dominated the scenario of automatic translation until the 1980s, when corpus-based (empirical) approaches emerged. Allied to the motivation of trying to overcome the limitations of rule-based translation, these approaches were driven by two factors: (1) advances in hardware necessary for heavier computational processing, and (2) greater availability of bilingual resources, especially parallel corpora. The next sections deal with corpus-based approaches: example-based translation, statistical translation, and neural translation. Example-based machine translation (EBMT), also known as analogy translation, is often associated with the publication of Nagao's article (1984), in which he proposes a model based on imitation of examples of translation of similar phrases, aiming to use the idea of learning to translate from existing examples (Koehn, 2020). Example-based systems use information extracted (word sequences) from examples in bilingual corpora of translation pairs, aligned at the sentence level, which has been conventionally called parallel corpora. Through this approach, examples such as those illustrated in Table 18.2 would serve as the basis for the system to learn translations of text excerpts, as illustrated in Table 18.3.                               \n",
            "From the learned excerpts, the example-based system would be able to combine them to generate the output presented in Example 18.2: Example 18.2 Input: My grandfather's house is beautiful. Output: The house_my.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sumarização + Perguntas e respostas**\n",
        "\n",
        "Nessa parte foi feita uma *gambiarra*:\n",
        "A API do ChatCompletions tem duas restrições, não pode enviar mais que 4096 caracteres, e também não podem ser feitas mais que 3 chamadas na mesma compilação.\n",
        "Então eu pedi para o texto ser sumarizado, e juntei os resultados em diferentes células, para depois, enviar o texto como referência, e ele usar esse texto como base para perguntas serem respondidas."
      ],
      "metadata": {
        "id": "KrtuXrCFd8NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "texto_sumarizado = \"\"\n",
        "\n",
        "for i in range (0,3):\n",
        "  texto = texto_completo[tamanho*i:tamanho*(i+1)]\n",
        "  openai.api_key = minha_chave\n",
        "\n",
        "  resposta = openai.Completion.create(\n",
        "    model = \"text-davinci-003\",\n",
        "    prompt = \"Resuma o seguinte texto:\"+texto,\n",
        "    temperature = 0,\n",
        "    max_tokens = 60\n",
        "  )\n",
        "\n",
        "  texto_resposta = resposta.choices[0].text\n",
        "  texto_sumarizado += texto_resposta"
      ],
      "metadata": {
        "id": "Z3eRX5W1PCdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (3,6):\n",
        "  texto = texto_completo[tamanho*i:tamanho*(i+1)]\n",
        "  openai.api_key = minha_chave\n",
        "\n",
        "  resposta = openai.Completion.create(\n",
        "    model = \"text-davinci-003\",\n",
        "    prompt = \"Resuma o seguinte texto:\"+texto,\n",
        "    temperature = 0,\n",
        "    max_tokens = 60\n",
        "  )\n",
        "\n",
        "  texto_resposta = resposta.choices[0].text\n",
        "  texto_sumarizado += texto_resposta"
      ],
      "metadata": {
        "id": "VnO2KcWYXeZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (6,9):\n",
        "  texto = texto_completo[tamanho*i:tamanho*(i+1)]\n",
        "  openai.api_key = minha_chave\n",
        "\n",
        "  resposta = openai.Completion.create(\n",
        "    model = \"text-davinci-003\",\n",
        "    prompt = \"Resuma o seguinte texto:\"+texto,\n",
        "    temperature = 0,\n",
        "    max_tokens = 60\n",
        "  )\n",
        "\n",
        "  texto_resposta = resposta.choices[0].text\n",
        "  texto_sumarizado += texto_resposta"
      ],
      "metadata": {
        "id": "aBcGo3SaXkPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (9,12):\n",
        "  texto = texto_completo[tamanho*i:tamanho*(i+1)]\n",
        "  openai.api_key = minha_chave\n",
        "\n",
        "  resposta = openai.Completion.create(\n",
        "    model = \"text-davinci-003\",\n",
        "    prompt = \"Resuma o seguinte texto:\"+texto,\n",
        "    temperature = 0,\n",
        "    max_tokens = 60\n",
        "  )\n",
        "\n",
        "  texto_resposta = resposta.choices[0].text\n",
        "  texto_sumarizado += texto_resposta"
      ],
      "metadata": {
        "id": "uTCR_ZuAbl88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (12,15):\n",
        "  texto = texto_completo[tamanho*i:tamanho*(i+1)]\n",
        "  openai.api_key = minha_chave\n",
        "\n",
        "  resposta = openai.Completion.create(\n",
        "    model = \"text-davinci-003\",\n",
        "    prompt = \"Resuma o seguinte texto:\"+texto,\n",
        "    temperature = 0,\n",
        "    max_tokens = 60\n",
        "  )\n",
        "\n",
        "  texto_resposta = resposta.choices[0].text\n",
        "  texto_sumarizado += texto_resposta"
      ],
      "metadata": {
        "id": "_4BcIP16cWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_sumarizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3110wqnrdWEy",
        "outputId": "2390ede2-5c8d-486b-b99d-7935adb37cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "izar a tradução de um texto, considerando a estrutura da língua-fonte e gerando a estrutura da língua-alvo.\n",
            "\n",
            "A tradução automática (TA) é oordagem é realmente viável?\n",
            "\n",
            "A tradução automática baseada em regras é um processo automático mais complexo do que a tradução direta, pois é necessário determinar o \"p grandfather is beautiful.\n",
            "\n",
            "O paradigma linguístico (baseado em regras e interlíngua) dominou o cenário da tradução automática até a década de 1980, quando abordagens baseadas em a tradução gerada.\n",
            "\n",
            "Os sistemas de TA estatísticos (SMT) usam modelos estatísticos para extrair pares de tradução de corpora bilíngues. Esses modelos são usazão de aprendizado. A tradução neural usa redes neurais artificiais para mapear textos paralelos alinhados e são treinadas para maximizar a probabilidade de uma sequência aliga traduzi-la.\n",
            "\n",
            "A Tradução Neural (NMT) é uma abordagem de tradução automática baseada em redes neurais artificiais. Ela usa diversas camadas de neurônios paraões aceitáveis para um texto de entrada” (Doherty et al., 2018, p. 7).\n",
            "\n",
            "A avaliação da Tradução Automática (ATA) é a prática de analisar a sa avaliação de TA, como as diretrizes de avaliação de TA da WMT (WMT Evaluation Guidelines, 2019).\n",
            "\n",
            "A avaliação da TA tem sido realizada desde o surgimento da própria TA. Nose a BERTScore (Zhang et al., 2019) e a métrica de similaridade semântica (SMS) (Chen et al., 2020).\n",
            "\n",
            "A avaliação de TA é um campo complexo que requer a abordag e é medida como a porcentagem de palavras corretamente traduzidas. Existem diversas métricas automáticas para medir a adequação, como MEANT (word embedding) (Lo; Wu, 2011 permitindo ao avaliador classificar as traduções de acordo com a sua qualidade.\n",
            "\n",
            "A avaliação humana da Tradução Automática (TA) é dividida em duas dimensix). O ranqueamento é uma técnica de avaliação de TA que permite a hierarquização das traduções, enquanto a anotação de erros é uma técnica que identificlise de todos os parágrafos de um artigo.\n",
            "\n",
            "A PE (Pós-Edição) é uma ferramenta importante para avaliar sistemas de TA (Tradução Automática), poão mais profunda da qualidade de TA.\n",
            "\n",
            "Barrault et al. (2020) estudaram a análise de segmentos específicos enquanto visualizavam o documento completo, bem como a avaliaç A TA (Tradução Automática) está se tornando cada vez mais importante com a globalização e a internet, pois mais conteúdo é criado todos os dias. A TA neural (NMT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Voce é um assistente virtual, e sua base de conhecimento é esse texto:\"+texto_sumarizado+\", voce utilizazrá ele para responder perguntas que forem feitas a respeito do texto\"},\n",
        "            {\"role\": \"user\", \"content\": \"Voce acredita que a Tradução automática é realmente viavel?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzOUDLU7cd2y",
        "outputId": "b4be3e97-fe70-4862-fd59-a3cf6faec75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A tradução automática é uma abordagem viável para a tradução de texto, e tem evoluído ao longo do tempo com o desenvolvimento de sistemas baseados em regras, estatísticas e redes neurais artificiais. No entanto, a qualidade da tradução automática ainda varia dependendo do contexto e da linguagem envolvida. A avaliação da tradução automática é fundamental para compreender sua eficácia, e diferentes métricas estão sendo utilizadas para isso. A pós-edição, que envolve a correção manual de traduções automáticas, também desempenha um papel importante na melhoria da qualidade da tradução automática. No contexto da globalização e da quantidade crescente de conteúdo online, a tradução automática se torna cada vez mais relevante.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Voce é um assistente virtual, e sua base de conhecimento é esse texto:\"+texto_sumarizado+\", voce utilizazrá ele para responder perguntas que forem feitas a respeito do texto\"},\n",
        "            {\"role\": \"user\", \"content\": \"O que você pode falar sobre a pós edição (PE)\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KLzA-WvdZQF",
        "outputId": "2bdde64a-c291-4a0c-f660-e5153b1d72d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto traduzido:\n",
            "\n",
            " A Pós-Edição (PE) é uma ferramenta importante para avaliar sistemas de Tradução Automática (TA). Consiste na revisão e edição do texto traduzido por um sistema de TA, feita por um tradutor humano. A PE busca aprimorar a qualidade da tradução automática, corrigindo possíveis erros ou imperfeições e garantindo que o texto final atenda aos requisitos de qualidade e precisão necessários.\n",
            "\n",
            "Barrault et al. (2020) realizaram estudos sobre a análise de segmentos específicos durante a visualização do documento completo, além da avaliação global da tradução. A PE é essencial para uma avaliação mais profunda da qualidade da TA, permitindo a identificação e correção de questões que podem não ser captadas por métodos automáticos de avaliação.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "\n",
        "openai.api_key = minha_chave\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Voce é um assistente virtual, e sua base de conhecimento é esse texto:\"+texto_sumarizado+\", voce utilizazrá ele para responder perguntas que forem feitas a respeito do texto\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explique pra mim sobre do que se trata o texto\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "texto_resposta = response['choices'][0]['message']['content']\n",
        "\n",
        "print(texto_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaL7gKjidrND",
        "outputId": "1e3f5916-0788-484d-a064-8883973fcaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O texto trata da tradução automática (TA) e das diferentes abordagens e métodos utilizados nesse campo. Ele discute as abordagens baseadas em regras, em modelos estatísticos e em redes neurais artificiais para realizar a tradução automática. Além disso, o texto aborda a avaliação da tradução automática, incluindo a avaliação automática e a avaliação humana, com diferentes métricas e técnicas. Também menciona a importância da pós-edição (PE) na avaliação da qualidade da tradução automática. Em resumo, o texto fornece uma visão geral do cenário da tradução automática e destaca a crescente importância desse campo devido à globalização e ao aumento do conteúdo digital.\n"
          ]
        }
      ]
    }
  ]
}